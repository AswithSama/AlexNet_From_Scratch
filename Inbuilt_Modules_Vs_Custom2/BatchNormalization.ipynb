{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "1. Class Initialization:\n",
    "- The __init__ method initializes epsilon for numerical stability and momentum to control the update rate for running statistics.\n",
    "2. Parameter Initialization:\n",
    "- The initialize_params method initializes the learnable parameters gamma (scaling factor) and beta (shift), as well as running_mean and running_var for inference.\n",
    "3. Forward Pass (Normalization):\n",
    "- If training=True, the batch mean and variance are calculated on the current batch.\n",
    "These statistics are used to normalize the batch, and the running mean and variance are updated using momentum.\n",
    "- If training=False, we use the running mean and variance accumulated during training for normalization.\n",
    "- Finally, the normalized values are scaled by gamma and shifted by beta\n",
    "4. Updating Parameters:\n",
    "- The update_params method allows updating gamma and beta, which is useful in a training scenario with backpropagation.\n",
    "5. Example Usage:\n",
    "- We create a simulated batch of input data and apply the batch normalization during both training and inference phases to observe the differences.\n",
    "Key Differences from TensorFlow's BatchNormalization\n",
    "- Backpropagation and Parameter Updates: In a complete neural network training process, gamma and beta would be updated during backpropagation. This code does not include backpropagation; however, the update_params method could be connected to an optimizer.\n",
    "- Momentum and Running Statistics: This example uses the momentum parameter to update running_mean and running_var, similar to TensorFlow's implementation, which uses these statistics during inference.\n",
    "This custom BatchNormalization layer can be integrated into a larger neural network framework or used for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Initializes the BatchNormalization layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - epsilon (float): Small constant to avoid division by zero.\n",
    "        - momentum (float): Momentum for the running mean and variance.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "\n",
    "    def initialize_params(self, shape):\n",
    "        \"\"\"\n",
    "        Initializes gamma and beta parameters, as well as running mean and variance.\n",
    "        \n",
    "        Parameters:\n",
    "        - shape (tuple): Shape of the input data (features).\n",
    "        \"\"\"\n",
    "        # Initialize scale (gamma) and shift (beta) parameters\n",
    "        self.gamma = np.ones(shape)\n",
    "        self.beta = np.zeros(shape)\n",
    "        \n",
    "        # Initialize running statistics for mean and variance\n",
    "        self.running_mean = np.zeros(shape)\n",
    "        self.running_var = np.ones(shape)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"\n",
    "        Applies batch normalization to the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (np.array): Input data with shape (batch_size, features).\n",
    "        - training (bool): Indicates whether to use batch statistics or running statistics.\n",
    "        \n",
    "        Returns:\n",
    "        - np.array: The batch-normalized data.\n",
    "        \"\"\"\n",
    "        # Initialize parameters if not done already\n",
    "        if self.gamma is None or self.beta is None:\n",
    "            self.initialize_params(X.shape[1:])\n",
    "\n",
    "        if training:\n",
    "            # Calculate batch mean and variance\n",
    "            batch_mean = np.mean(X, axis=0)\n",
    "            batch_var = np.var(X, axis=0)\n",
    "            \n",
    "            # Normalize the input\n",
    "            X_norm = (X - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift the normalized data\n",
    "        out = self.gamma * X_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def update_params(self, gamma, beta):\n",
    "        \"\"\"\n",
    "        Updates gamma and beta parameters for backpropagation.\n",
    "        \n",
    "        Parameters:\n",
    "        - gamma (np.array): Learned scale parameter (of same shape as features).\n",
    "        - beta (np.array): Learned shift parameter (of same shape as features).\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([96, 55, 55])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Example input tensor\n",
    "X = tf.random.normal((96,55, 55))\n",
    "\n",
    "# Custom BatchNormalization layer\n",
    "bn_custom = BatchNormalization()\n",
    "output_custom = bn_custom.forward(X, training=True)\n",
    "output_custom.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
